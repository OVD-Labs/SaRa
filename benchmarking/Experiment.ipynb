{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import saraRC1 as sara\n",
    "from time import time\n",
    "\n",
    "import scipy.stats as sc\n",
    "\n",
    "seg_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.001\n",
    "\n",
    "def index_to_coordinates(index, seg_dim, im_size):\n",
    "    '''\n",
    "    Given an index and a shape, this function returns the corresponding coordinates.\n",
    "    '''\n",
    "\n",
    "    x1 = int((index % seg_dim) * (im_size[1] / seg_dim))\n",
    "    y1 = int((index // seg_dim) * (im_size[0] / seg_dim))\n",
    "\n",
    "    x2 = int(x1 + (im_size[1] / seg_dim))\n",
    "    y2 = int(y1 + (im_size[0] / seg_dim))\n",
    "    \n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def mae(pred, gt):\n",
    "    return np.mean(np.abs(pred - gt))\n",
    "\n",
    "experiment_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MASK R-CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthewkenely/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/matthewkenely/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/matthewkenely/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/matthewkenely/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/matthewkenely/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/matthewkenely/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                93\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    81\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "1.3.0\n",
      "2.0.8\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import h5py\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(h5py.__version__)\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepGaze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/matthewkenely/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "Using cache found in /home/matthewkenely/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "import deepgaze_pytorch\n",
    "DEVICE = 'cuda'\n",
    "deepgaze = deepgaze_pytorch.DeepGazeIIE(pretrained=True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results.csv\n",
    "if not os.path.exists('results.csv'):\n",
    "    with open('results.csv', 'w') as f:\n",
    "        f.write('Experiment,SOR,MAE\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT: 1\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "--> Processing image 1 of 5 (deepgaze) [20.0%]\n",
      "Time for image 1: 0.9567081928253174 seconds\n",
      "--> Processing image 2 of 5 (deepgaze) [40.0%]\n",
      "Time for image 2: 0.838491678237915 seconds\n",
      "--> Processing image 3 of 5 (deepgaze) [60.0%]\n",
      "Time for image 3: 0.8379702568054199 seconds\n",
      "--> Processing image 4 of 5 (deepgaze) [80.0%]\n",
      "Time for image 4: 0.8506133556365967 seconds\n",
      "--> Processing image 5 of 5 (deepgaze) [100.0%]\n",
      "Time for image 5: 0.8434844017028809 seconds\n",
      "PR: [2, 1, 4, 3]\n",
      "GT: [2, 1, 5, 4]\n",
      "\n",
      "PR: [1, 2, 3]\n",
      "GT: [1, 2, 3]\n",
      "\n",
      "PR: [3, 2, 1]\n",
      "GT: [3, 4, 1]\n",
      "\n",
      "PR: [3, 2, 1]\n",
      "GT: [2, 5, 3]\n",
      "\n",
      "PR: [3, 1, 2]\n",
      "GT: [5, 1, 2]\n",
      "\n",
      "Average SPR: 0.6\n",
      "Average /mae: 0.6348834635416667\n",
      "EXPERIMENT: 2\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0a5110f8a95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Run detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programming/Work/uni/Summer Placement/SaRa/benchmarking/mrcnn/model.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, images, verbose)\u001b[0m\n\u001b[1;32m   2522\u001b[0m         \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m         \u001b[0;31m# Process detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sara/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ranges = list(zip(np.arange(1, 7000, 100), np.arange(100, 7100, 100)))\n",
    "ranges = [list(t) for t in ranges]\n",
    "ranges[0][0] = 0\n",
    "ranges[0][1] = 5\n",
    "\n",
    "img_path = './Siris Dataset/ASSR/images/train/'\n",
    "\n",
    "for experiment in range(1, 10):\n",
    "    print('EXPERIMENT:', experiment)\n",
    "    range_ = ranges[experiment - 1]\n",
    "    imgs = {}\n",
    "    i = 0\n",
    "\n",
    "    ### IMAGES\n",
    "    for root, dirs, files in os.walk(img_path):\n",
    "        for file in files[range_[0]:range_[1]]:\n",
    "            file_name = file.split('.')[0]\n",
    "            imgs[file_name] = cv2.cvtColor(cv2.imread(os.path.join(root, file)), cv2.COLOR_BGR2RGB)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    ### MASKS\n",
    "    mask_path = './Siris Dataset/ASSR/gt/train/'\n",
    "    gt_masks = {}\n",
    "    gt_ranks = {}\n",
    "\n",
    "    for file_name in imgs:\n",
    "        gt_ranks[file_name] = {}\n",
    "        \n",
    "        file = file_name + '.png'\n",
    "        mask = cv2.cvtColor(cv2.imread(os.path.join(mask_path, file)), cv2.COLOR_BGR2GRAY)\n",
    "        # masks[file_name] = cv2.cvtColor(cv2.imread(os.path.join(mask_path, file)), cv2.COLOR_BGR2GRAY)\n",
    "        # Separate the mask based on colour\n",
    "        # masks[file_name] = {}\n",
    "        gt_masks[file_name] = {}\n",
    "\n",
    "        # Detect different colours in mask\n",
    "        # Create histogram\n",
    "        hist = cv2.calcHist([mask], [0], None, [256], [0, 256])\n",
    "\n",
    "        # Show non-zero values and extract intensity values at that freq\n",
    "        non_zero = np.nonzero(hist)\n",
    "        x = non_zero[0][1:]\n",
    "\n",
    "        # Separate mask into regions which match the intensity values in x\n",
    "        for i, intensity in enumerate(reversed(x)):\n",
    "            # masks[file_name][i] = np.where(mask == intensity, 1, 0)\n",
    "            gt_masks[file_name][i] = np.where(mask == intensity, 1, 0)\n",
    "            \n",
    "            # Calculate ranks based on highest intensity\n",
    "            gt_ranks[file_name][i] = i + 1\n",
    "\n",
    "    # masks = gt_masks # To be replaced with MASK R-CNN\n",
    "    masks = {}\n",
    "\n",
    "    for file_name in imgs:\n",
    "        # Run detection\n",
    "        results = model.detect([imgs[file_name]], verbose=1)\n",
    "\n",
    "        masks[file_name] = {}\n",
    "        \n",
    "        for i in range(len(results[0]['class_ids'])):\n",
    "            masks[file_name][i] = np.array(results[0]['masks'][:, :, i], dtype=np.uint8) * 255\n",
    "\n",
    "    # Match masks generated to Mask RCNN to ground truth masks\n",
    "    masks_valid = {}\n",
    "    gt_masks_valid = {}\n",
    "    gt_ranks_valid = {}\n",
    "\n",
    "    for file_name in masks:\n",
    "        masks_valid[file_name] = {}\n",
    "        gt_masks_valid[file_name] = {}\n",
    "        gt_ranks_valid[file_name] = {}\n",
    "        \n",
    "        for mask in masks[file_name]:\n",
    "            best_iou = 0\n",
    "            for gt_mask in gt_masks[file_name]:\n",
    "                # Calculate IoU\n",
    "                intersection = np.logical_and(masks[file_name][mask], gt_masks[file_name][gt_mask])\n",
    "                union = np.logical_or(masks[file_name][mask], gt_masks[file_name][gt_mask])\n",
    "                iou_score = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "                if iou_score > 0.5 and iou_score > best_iou:\n",
    "                    best_iou = iou_score\n",
    "                    masks_valid[file_name][mask] = masks[file_name][mask]\n",
    "                    gt_masks_valid[file_name][mask] = gt_masks[file_name][gt_mask]\n",
    "                    gt_ranks_valid[file_name][mask] = gt_ranks[file_name][gt_mask]\n",
    "\n",
    "            # fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "            # ax[0].imshow(masks_valid[file_name][mask], cmap='gray')\n",
    "            # ax[1].imshow(gt_masks_valid[file_name][mask], cmap='gray')\n",
    "            # plt.show()\n",
    "\n",
    "    masks = masks_valid\n",
    "    gt_masks = gt_masks_valid\n",
    "    gt_ranks = gt_ranks_valid\n",
    "\n",
    "    \n",
    "    ### GENERATE SALIENCY MAPS, HEATMAPS, RANKINGS\n",
    "    saliency_maps = {}\n",
    "    sara_heatmaps = {}\n",
    "    sara_lists = {}\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # generators = ['itti', 'deepgaze', 'fpn', 'emlnet']\n",
    "    generators = ['deepgaze']\n",
    "\n",
    "    g = 1\n",
    "\n",
    "    for i, im in enumerate(imgs):\n",
    "        start = time()\n",
    "\n",
    "        if im not in saliency_maps:\n",
    "            saliency_maps[im] = {}\n",
    "            sara_heatmaps[im] = {}\n",
    "            sara_lists[im] = {}\n",
    "\n",
    "        for generator in generators:\n",
    "            percent = round(g/(len(imgs)*len(generators))*100, 2)\n",
    "            \n",
    "            print(f'--> Processing image {i+1} of {len(imgs)} ({generator}) [{percent}%]')\n",
    "\n",
    "            saliency_maps[im][generator] = sara.return_saliency(imgs[im].copy(), generator=generator, deepgaze_model=deepgaze)\n",
    "            sara.reset()\n",
    "\n",
    "            sara_heatmaps[im][generator], sara_lists[im][generator] = sara.return_sara(imgs[im].copy(), seg_dim, saliency_map=saliency_maps[im][generator])\n",
    "            sara.reset()\n",
    "\n",
    "            g += 1\n",
    "        \n",
    "        print(f'Time for image {i + 1}: {time() - start} seconds')\n",
    "\n",
    "    ### MASK RANKING\n",
    "    # For each segment, check which mask falls under that segment using MRn = rank(Gi); (Gi interesect Mn) > T\n",
    "    mask_segments = {}\n",
    "\n",
    "    for sara_list in sara_lists:\n",
    "        for segment in sara_lists[sara_list]['deepgaze']:\n",
    "            if sara_list not in mask_segments:\n",
    "                mask_segments[sara_list] = {}\n",
    "\n",
    "            # Convert index to coordinates, extract segment from heatmap\n",
    "            shape = sara_heatmaps[sara_list]['deepgaze'].shape[0:2]\n",
    "            x1, y1, x2, y2 = index_to_coordinates(segment[5], seg_dim, shape)\n",
    "            # print(x1, y1, x2, y2)\n",
    "\n",
    "            for m in masks[sara_list]:\n",
    "                if m not in mask_segments[sara_list]:\n",
    "                    mask_segments[sara_list][m] = []\n",
    "\n",
    "                # Extract mask from masks\n",
    "                mask = masks[sara_list][m][y1:y2, x1:x2]\n",
    "\n",
    "                # Calculate intersection over union\n",
    "                intersection = np.sum(mask > 0)\n",
    "                union = np.sum(mask > 0) + np.sum(mask == 0)\n",
    "\n",
    "                iou = intersection / union\n",
    "\n",
    "                # print('Segment: ', segment[5], 'Mask: ', m, 'IoU: ', iou)\n",
    "\n",
    "                if iou > T:\n",
    "                    # index, rank, saliency\n",
    "                    # print(segment)\n",
    "                    mask_segments[sara_list][m].append((segment[5], segment[0], segment[1]))\n",
    "                    # print(mask_segments)\n",
    "\n",
    "    # For each mask, find the segment with the lowest rank\n",
    "    mask_segments_min = {}\n",
    "\n",
    "    for sara_list in mask_segments:\n",
    "        for m in mask_segments[sara_list]:\n",
    "            # mask_segments_min[sara_list][m] = min(mask_segments[sara_list][m], key=lambda x: x[1])[0]\n",
    "            if sara_list not in mask_segments_min:\n",
    "                mask_segments_min[sara_list] = {}\n",
    "            \n",
    "            mask_segments_min[sara_list][m] = min(mask_segments[sara_list][m], key=lambda x: x[1])\n",
    "\n",
    "    mask_segments_min\n",
    "\n",
    "    mask_ranks = {}\n",
    "\n",
    "    for sara_list in mask_segments_min:\n",
    "        mask_ranks[sara_list] = {}\n",
    "        # Extract the ranks and sort them by the third value in each tuple\n",
    "        sorted_ranks = sorted(mask_segments_min[sara_list].items(), key=lambda x: x[1][1])\n",
    "\n",
    "        for i in range(len(sorted_ranks)):\n",
    "            mask_ranks[sara_list][sorted_ranks[i][0]] = i + 1\n",
    "\n",
    "        # Sort mask_ranks[sara_list] by object\n",
    "        mask_ranks[sara_list] = {k: v for k, v in sorted(mask_ranks[sara_list].items(), key=lambda item: item[0])}\n",
    "\n",
    "\n",
    "    ### SOR\n",
    "    all_spr = 0\n",
    "\n",
    "    for sara_list in mask_ranks:\n",
    "        print(f'PR: {list(mask_ranks[sara_list].values())}\\nGT: {list(gt_ranks[sara_list].values())}')\n",
    "        try:\n",
    "            spr = sc.spearmanr(list(mask_ranks[sara_list].values()), list(gt_ranks[sara_list].values()))\n",
    "            all_spr += spr.correlation\n",
    "        except:\n",
    "            pass\n",
    "        print()\n",
    "\n",
    "    print(f'Average SPR: {all_spr / len(mask_ranks)}')\n",
    "\n",
    "    all_mae = 0\n",
    "\n",
    "    for sara_list in masks:\n",
    "        for mask in masks[sara_list]:\n",
    "            # print(masks[sara_list][mask] - gt_masks[sara_list][mask])\n",
    "            pred = masks[sara_list][mask]\n",
    "            pred = np.array(pred, dtype=np.uint8)\n",
    "            pred = cv2.threshold(pred, 0.5, 1, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "            gt = gt_masks[sara_list][mask]\n",
    "            gt = np.array(gt, dtype=np.uint8)\n",
    "            gt = cv2.threshold(gt, 0.5, 1, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "            temp_mae = mae(pred, gt)\n",
    "            # plt.figure()\n",
    "            # plt.subplot(131)\n",
    "            # plt.imshow(pred)\n",
    "            # plt.subplot(132)\n",
    "            # plt.imshow(gt)\n",
    "            # plt.subplot(133)\n",
    "            # plt.imshow(pred - gt)\n",
    "            all_mae += temp_mae\n",
    "            # print()\n",
    "            break\n",
    "\n",
    "    print(f'Average /mae: {all_mae / len(masks)}')\n",
    "\n",
    "    experiment_results[experiment] = {}\n",
    "    experiment_results[experiment]['SOR'] = all_spr / len(mask_ranks)\n",
    "    experiment_results[experiment]['MAE'] = all_mae / len(masks)\n",
    "\n",
    "    # Append to results.csv\n",
    "    with open('results.csv', 'a') as f:\n",
    "        f.write(f'{experiment},{experiment_results[experiment][\"SOR\"]},{experiment_results[experiment][\"MAE\"]}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f68281c7a20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(masks_valid[list(masks_valid.keys())[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f68281445c0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(gt_masks_valid[list(gt_masks_valid.keys())[0]][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
